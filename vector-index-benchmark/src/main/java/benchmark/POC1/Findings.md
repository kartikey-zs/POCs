# GG-47182 POC-1: JVector Cleanup Behavior — Investigation Report

---

## Overview

This document reports findings from a hands-on investigation of JVector's `cleanup()` behavior under realistic deletion patterns. The goal was to validate JVector's documented claim that cleanup runtime scales with deleted node count, understand ordinal space behavior, measure memory characteristics, and determine whether the index remains searchable during compaction.

---

## Setup

The investigation used two datasets:

**SIFT-1M (128d)** — 1M synthetic vectors, 128 dimensions, 10k queries, exact ground truth. Used as the primary benchmark dataset for comparison across M=16 and M=32.

**Cohere Wikipedia (1024d)** — 500k real Wikipedia article embeddings generated by Cohere's embed-multilingual-v3 model, 1024 dimensions, 10k queries, exact ground truth computed via FAISS brute force. Used to validate findings at realistic production embedding dimensions.

All tests were run on a MacBook Pro with 24GB RAM and 12 CPU cores. JVector was configured with M=16, efConstruction=100, efSearch=200 unless stated otherwise. Deletion was performed in 9 equal waves of 10% each, accumulating from 10% to 90% cumulative deletion. Recall was computed at k=10 using filtered ground truth — ground truth vectors that had been deleted were excluded before computing recall, isolating graph quality degradation from the effect of missing vectors.

---

## Finding 1: Cleanup Runtime is NOT Proportional to Deleted Nodes

JVector's documentation states: *"runtime proportional to the number of deleted nodes."* This is incorrect under realistic conditions.

### SIFT-1M, M=16 — Cleanup time per wave

| Cumulative deleted | Live nodes | Cleanup time |
|---|---|---|
| 10% (100k deleted) | 900,000 | 24,000 ms |
| 20% (200k deleted) | 800,000 | **30,000 ms** ← peak |
| 30% (300k deleted) | 700,000 | 25,000 ms |
| 40% (400k deleted) | 600,000 | 24,000 ms |
| 50% (500k deleted) | 500,000 | 21,000 ms |
| 60% (600k deleted) | 400,000 | 23,000 ms |
| 70% (700k deleted) | 300,000 | 19,000 ms |
| 80% (800k deleted) | 200,000 | 15,000 ms |
| 90% (900k deleted) | 100,000 | 8,000 ms |

At 90% cumulative deletion there are 9× more deleted nodes than at 10%, yet cleanup takes 3× less time. This directly contradicts the documented claim.

The real cost driver is:

```

cleanup_cost ≈ deleted_nodes × avg_neighbors × search_cost_through_live_graph

```

The third term — search cost through the live graph — dominates and shrinks dramatically as live node density falls. When only 100k live nodes remain, rewiring each deleted node's neighbors is trivial because the graph is sparse and traversal is cheap. Cleanup cost is therefore primarily a function of live node density, not deleted node count.

The practical implication is that cleanup cost peaks early — at approximately 20% cumulative deletion — and then monotonically decreases. Triggering cleanup before the 20% threshold is optimal.

### Cohere 1024d, M=16 — Cleanup time per wave

| Cumulative deleted | Live nodes | Cleanup time |
|---|---|---|
| 10% | 450,000 | 13,815 ms |
| 20% | 400,000 | **16,994 ms** ← peak |
| 30% | 350,000 | 15,542 ms |
| 40% | 300,000 | 15,060 ms |
| 50% | 250,000 | 12,055 ms |
| 60% | 200,000 | 12,670 ms |
| 70% | 150,000 | 11,212 ms |
| 80% | 100,000 | 8,637 ms |
| 90% | 50,000 | 4,827 ms |

The same pattern holds at 1024d — peak at 20%, monotonic decrease thereafter.

---

## Finding 2: M=32 Multiplies Cleanup Cost by ~3×

### SIFT-1M — M=16 vs M=32

| Cumulative deleted | M=16 cleanup | M=32 cleanup | Ratio |
|---|---|---|---|
| 10% | 24,000 ms | 64,365 ms | 2.7× |
| 20% | 30,000 ms | **88,288 ms** | 2.9× |
| 30% | 25,000 ms | 65,644 ms | 2.6× |
| 40% | 24,000 ms | 74,102 ms | 3.1× |
| 50% | 21,000 ms | 61,650 ms | 2.9× |
| 60% | 23,000 ms | 70,720 ms | 3.1× |
| 70% | 19,000 ms | 58,758 ms | 3.1× |
| 80% | 15,000 ms | 48,167 ms | 3.2× |
| 90% | 8,000 ms | 22,792 ms | 2.9× |

The ~3× multiplier is consistent across waves. M=32 increases both neighbor rewiring count and graph density, compounding cost.

M=32 does deliver measurably better recall (>0.993 across healthy waves) compared to M=16 (~0.984), but both exhibit a recall cliff at 80%+ deletion.

---

## Finding 3: Recall is Resilient Through 60–70% Deletion

### SIFT-1M, M=16

| Cumulative deleted | Recall before cleanup | Recall after cleanup |
|---|---|---|
| 10% | 0.9840 | 0.9825 |
| 20% | 0.9843 | 0.9830 |
| 30% | 0.9846 | 0.9835 |
| 40% | 0.9852 | 0.9837 |
| 50% | 0.9854 | 0.9851 |
| 60% | 0.9847 | 0.9843 |
| 70% | 0.9814 | 0.9807 |
| 80% | 0.9494 | 0.9498 |
| 90% | 0.7601 | 0.7617 |

Recall remains above 0.980 through 70% deletion. The cliff begins at 80% and collapses at 90%.

### Cohere 1024d, M=16

Recall degradation begins earlier (around 70%), with significant drop at 80% (~0.919).

Implication: deletion ratio is a reliable cleanup trigger metric. Scheduling cleanup at 15–20% maintains graph health and avoids peak compaction cost.

---

## Finding 4: Ordinal Space Never Compacts

JVector does not support ordinal remapping. Ordinals are sequential `int` values assigned via `AtomicInteger maxNodeId`. When nodes are deleted and cleanup runs:

- The `Neighbors` object is removed from `ConcurrentNeighborMap`.
- Heap memory for adjacency lists is reclaimed.
- Ordinal values are never reused.
- `getIdUpperBound()` always returns `maxNodeId + 1`.

### SIFT-1M at 90% deletion

| | Upper bound | Live nodes | Holes | Fragmentation |
|---|---|---|---|---|
| Before cleanup | 1,000,000 | 200,000 | 800,000 | 80% |
| After cleanup | 1,000,000 | 100,000 | 900,000 | 90% |

Cleanup still scans the full ordinal range regardless of live count.

High-churn partitions will experience monotonic `maxNodeId` growth proportional to total historical insertions. Long-term mitigation requires full index rebuild. Blue-green architecture is appropriate for zero-downtime rebuild.

---

## Finding 5: Searches Are Never Blocked During Cleanup

Cleanup blocks only the calling thread. Search threads continue concurrently.

- `removeDeletedNodes()` is `synchronized` on `GraphIndexBuilder`.
- Searches use `GraphSearcher`.
- `ConcurrentNeighborMap` enables fine-grained per-node locking.

Effect: P99 latency spikes, not blocking.

### SIFT-1M, M=16

| Phase | P50 | P99 | Recall |
|---|---|---|---|
| Before cleanup (50%) | 1,054 μs | 1,902 μs | 0.9813 |
| During cleanup (50%) | 985 μs | **9,663 μs** (5×) | 0.9824 |
| After cleanup (50%) | 757 μs | 1,073 μs | 0.9849 |
| Before cleanup (90%) | 1,049 μs | 1,638 μs | 0.9765 |
| During cleanup (90%) | 965 μs | **3,688 μs** (2.2×) | 0.9819 |
| After cleanup (90%) | 503 μs | 716 μs | 0.9907 |

### Cohere 1024d, M=16

| Phase | P50 | P99 | Recall |
|---|---|---|---|
| Before cleanup (50%) | 2,218 μs | 3,313 μs | 0.9705 |
| During cleanup (50%) | 2,199 μs | **12,881 μs** (3.9×) | 0.9707 |
| After cleanup (50%) | 1,615 μs | 2,381 μs | 0.9697 |
| Before cleanup (90%) | 2,336 μs | 3,401 μs | 0.9724 |
| During cleanup (90%) | 2,287 μs | **10,644 μs** (3.1×) | 0.9772 |
| After cleanup (90%) | 1,270 μs | 2,124 μs | 0.9809 |

Cleanup is safe for background execution. Expect 3–5× P99 spike during compaction window.

---

## Finding 6: Memory Behavior

Memory reclamation comes from removing `Neighbors` entries from `ConcurrentNeighborMap`.

### SIFT-1M, M=16

| Cumulative deleted | Before cleanup | After cleanup | Reclaimed |
|---|---|---|---|
| 10% | 1,935 MB | 1,486 MB | 449 MB |
| 20% | 2,798 MB | 1,521 MB | 1,277 MB |
| 40% | 2,830 MB | 1,472 MB | 1,358 MB |
| 80% | 2,763 MB | 1,350 MB | 1,413 MB |
| 90% | 1,622 MB | 1,307 MB | 315 MB |

### Cohere 1024d

Heap remains in 4,100–5,500 MB range due to raw vector storage:

- 500k × 1024d × 4 bytes ≈ 2 GB (vectors only)

Cleanup reclaims 500–1,400 MB per wave.

---

## Summary of Recommendations for GridGain RFC

- **Cleanup trigger**: 15–20% deletion.
- **Concurrent safety**: Safe to run with searches. Document expected P99 spike (3–5×).
- **Default M**: 16.  
  - >0.98 recall through 70% deletion  
  - ~1.8× faster build than M=32  
  - ~3× faster cleanup than M=32  
- **M=32**: Advanced configuration for >0.99 recall use cases.
- **Ordinal bloat mitigation**: Define maximum lifetime insertion threshold per partition. Trigger full rebuild beyond threshold.
- **Recall cliff**: Hard enforcement before 80% deletion. Recommended 15–20% trigger provides margin.